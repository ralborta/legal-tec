# üì° Scraping de URLs para RAG

## üéØ Descripci√≥n

Sistema para scrapear URLs p√∫blicas y a√±adirlas a la base de conocimiento del RAG. Permite acceder a contenido web en tiempo real y guardarlo para uso en generaci√≥n de documentos.

## üöÄ Uso

### 1. Scrapear URLs Nacionales (Script Predefinido)

Las 8 URLs nacionales argentinas est√°n predefinidas. Para scrapearlas y guardarlas:

```bash
npm run seed-urls
```

Esto:
- Crea la base de conocimiento `normativa_nacional_urls`
- Scrapea las 8 URLs nacionales
- Genera embeddings y los guarda en la DB
- Las hace disponibles para el RAG

### 2. Scrapear URLs Personalizadas (API)

```bash
POST /api/scrape-urls
Content-Type: application/json

{
  "urls": [
    "https://ejemplo.com/pagina1",
    "https://ejemplo.com/pagina2"
  ],
  "knowledgeBaseId": "mi_base_urls",
  "sourceType": "normativa"
}
```

### 3. Probar Scraping de una URL (Sin Guardar)

```bash
POST /api/scrape-url
Content-Type: application/json

{
  "url": "https://ejemplo.com/pagina"
}
```

Respuesta:
```json
{
  "url": "https://ejemplo.com/pagina",
  "title": "T√≠tulo de la p√°gina",
  "text": "Contenido extra√≠do...",
  "success": true
}
```

## üìã URLs Nacionales Predefinidas

1. **Bolet√≠n Oficial**: `https://www.boletinoficial.gob.ar/`
2. **InfoLEG**: `https://www.argentina.gob.ar/normativa`
3. **SIPROJUD**: `http://www.csjn.gov.ar/siprojur/`
4. **C√≥digo Civil y Comercial**: `http://www.bibliotecadigital.gob.ar/items/show/2690`
5. **C√≥digo Procesal**: `https://www.saij.gob.ar/7425-local-buenos-aires-codigo-procesal-civil-comercial-buenos-aires-lpb0007425-1968-09-19/123456789-0abc-defg-524-7000bvorpyel`
6. **SAIJ Jurisprudencia**: `https://www.argentina.gob.ar/justicia/saij`
7. **C√°mara de Diputados**: `https://www.hcdn.gob.ar/`
8. **Senado**: `https://www.senado.gob.ar/`

## üîß Caracter√≠sticas

### Extracci√≥n Inteligente
- Detecta autom√°ticamente el contenido principal
- Remueve scripts, estilos, navegaci√≥n, etc.
- Limita a 50K caracteres por p√°gina
- Limpia espacios y saltos de l√≠nea

### Rate Limiting
- Delay de 1 segundo entre requests
- Timeout de 30 segundos por URL
- Manejo de errores HTTP

### Almacenamiento
- Guarda en tabla `chunks` con metadata
- Asocia a base de conocimiento espec√≠fica
- Genera embeddings autom√°ticamente
- Incluye timestamp de scraping

## üí° Uso en Generaci√≥n de Documentos

Una vez scrapeadas, las URLs est√°n disponibles en el RAG:

1. **En el Frontend**: Seleccion√° la base de conocimiento `normativa_nacional_urls` al generar
2. **Via API**: Us√° `knowledgeBases: ["normativa_nacional_urls"]` en `/v1/generate`

## üîÑ Actualizaci√≥n

Para actualizar el contenido:

```bash
# Re-ejecutar el script (sobrescribe contenido existente)
npm run seed-urls
```

O usar el endpoint API con las URLs que quer√©s actualizar.

## ‚ö†Ô∏è Limitaciones

- Solo URLs p√∫blicas (sin autenticaci√≥n)
- Contenido est√°tico (no JavaScript din√°mico)
- M√°ximo 50K caracteres por p√°gina
- Rate limiting: 1 segundo entre requests
- Timeout: 30 segundos por URL

## üõ†Ô∏è Troubleshooting

### Error: "HTTP 403"
- El sitio puede estar bloqueando bots
- Verificar User-Agent en el c√≥digo

### Error: "Timeout"
- La p√°gina tarda mucho en cargar
- Considerar aumentar timeout o usar Puppeteer

### Contenido vac√≠o
- La p√°gina puede requerir JavaScript
- Considerar usar Puppeteer para p√°ginas din√°micas

## üìù Notas

- El scraping se hace una vez y se guarda en DB
- Para contenido siempre actualizado, re-ejecutar peri√≥dicamente
- Los embeddings se generan autom√°ticamente al guardar
- El contenido est√° disponible inmediatamente despu√©s del scraping










